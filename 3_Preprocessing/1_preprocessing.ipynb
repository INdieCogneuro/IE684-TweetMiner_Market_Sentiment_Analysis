{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3457304",
   "metadata": {},
   "source": [
    "- Merge All the data which download in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f719b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "folder_path = \"../2_data/NVDA_2013_2022\" \n",
    "\n",
    "\n",
    "csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "\n",
    "df_list = []\n",
    "\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\" Error reading {file}: {e}\")\n",
    "\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "merged_df.to_csv(\"../2_data/merged_nvda.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521ecfb",
   "metadata": {},
   "source": [
    "- Just run second block of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f6db6e-d8cf-4d14-985e-7dd815e88b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/q/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/q/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/q/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/q/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/q/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/q/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbafa4a4-1a5b-4d84-a76b-48ab4b8213cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finally', 'my', 'issue', 'of', 'nltk', 'is', 'resolved']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Finally my issue of nltk is resolved\"\n",
    "tokens = word_tokenize(text,language='english', preserve_line=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba28410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/q/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/q/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/rn/lc8n68_d7ld6yymqc_lsnrr00000gn/T/ipykernel_31872/436574755.py:24: DtypeWarning: Columns (14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../2_data/merged_nvda.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body  \\\n",
      "0         $NVDA going in for some weekly calls here.   \n",
      "1  NVIDIA&#39;s PT raised by Evercore ISI to $250...   \n",
      "2  Wells Fargo Maintains Overweight on NVIDIA, Ra...   \n",
      "3                500 $NVDA AUG2021 $205 Cs trade 3.1   \n",
      "4                                $NVDA Great volume.   \n",
      "\n",
      "                 created_at  \\\n",
      "0 2021-08-12 13:46:00+00:00   \n",
      "1 2021-08-12 13:45:26+00:00   \n",
      "2 2021-08-12 13:45:12+00:00   \n",
      "3 2021-08-12 13:43:30+00:00   \n",
      "4 2021-08-12 13:43:16+00:00   \n",
      "\n",
      "                                             symbols  \\\n",
      "0  [{'id': 2925, 'symbol': 'NVDA', 'title': 'NVID...   \n",
      "1  [{'id': 2925, 'symbol': 'NVDA', 'title': 'NVID...   \n",
      "2  [{'id': 2925, 'symbol': 'NVDA', 'title': 'NVID...   \n",
      "3  [{'id': 2925, 'symbol': 'NVDA', 'title': 'NVID...   \n",
      "4  [{'id': 2925, 'symbol': 'NVDA', 'title': 'NVID...   \n",
      "\n",
      "                                         clean_tweet  \n",
      "0           nvda going in for some weekly calls here  \n",
      "1  nvidia39s pt raised by evercore isi to 25000 o...  \n",
      "2  wells fargo maintains overweight on nvidia rai...  \n",
      "3                   500 nvda aug2021 205 cs trade 31  \n",
      "4                                  nvda great volume  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_tweet(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)  # drop http (they are just ad)\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # drop @\n",
    "    text = re.sub(r\"#\", \"\", text)  # keep hashtag\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # drop punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # drop unnecessary blank\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv(\"../2_data/merged_nvda.csv\")\n",
    "keep_cols = ['body', 'created_at', 'symbols']\n",
    "existing_cols = [col for col in keep_cols if col in df.columns]\n",
    "df = df[existing_cols].copy()\n",
    "\n",
    "\n",
    "df[\"clean_tweet\"] = df[\"body\"].apply(clean_tweet)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "# Keep records containing only NVDA (if symbols are str or list)\n",
    "df['symbols'] = df['symbols'].astype(str)\n",
    "df = df[df['symbols'].str.contains(\"NVDA\")]\n",
    "\n",
    "print(df.head())\n",
    "df.to_csv(\"../2_data/nvidia_tweets_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734390d4",
   "metadata": {},
   "source": [
    "- Most tweets are just spam, ads...so build a simple filter to get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60fca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/q/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/q/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/rn/lc8n68_d7ld6yymqc_lsnrr00000gn/T/ipykernel_31872/2154528657.py:46: DtypeWarning: Columns (14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../2_data/merged_nvda.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalï¼š584181ï¼Œafter filteringï¼š513285ï¼Œdropï¼š70896\n",
      "saveï¼šnvidia_tweets_filtered_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append('/Users/q/nltk_data') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# keyword filter\n",
    "advertising_keywords = [\n",
    "    \"learn trading\", \"get alerts\", \"get ideas\", \"get updates\", \"get analysis\",\n",
    "    \"welcome to discord\", \"welcome to the discord group\", \"bitcoin\", \"crypto\",\n",
    "    \"top analyst price target\", \"top analyst target price\", \"top analyst target for next week\",\n",
    "    \"ðŸŽ¯\", \"ðŸ“ˆ\", \"ðŸ“‰\", \"ðŸš€\"\n",
    "]\n",
    "\n",
    "def is_advertising(text: str) -> bool:\n",
    "    \"\"\"if contains https or any keywords in filterï¼Œtake it as ad\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    txt = text.lower()\n",
    "    if 'https' in txt:\n",
    "        return True\n",
    "    return any(keyword in txt for keyword in advertising_keywords)\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)  # drop links\n",
    "    text = re.sub(r\"www\\.\\S+\", \"\", text)        # drop links\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)            # drop @mention\n",
    "    text = re.sub(r\"#\", \"\", text)               # drop # but keep hashtag text\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)         # drop punctuation and Emoji\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()    # drop unnecessary blank\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "df = pd.read_csv(\"../2_data/merged_nvda.csv\")\n",
    "\n",
    "# filter\n",
    "before_count = len(df)\n",
    "df = df[~df[\"body\"].apply(is_advertising)].copy()\n",
    "after_count = len(df)\n",
    "print(f\"totalï¼š{before_count}ï¼Œafter filteringï¼š{after_count}ï¼Œdropï¼š{before_count - after_count}\")\n",
    "\n",
    "# datetime\n",
    "df[\"clean_tweet\"] = df[\"body\"].apply(clean_tweet)\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors=\"coerce\").dt.date\n",
    "\n",
    "df.to_csv(\"../2_data/nvidia_tweets_filtered_cleaned.csv\", index=False)\n",
    "print(\"saveï¼šnvidia_tweets_filtered_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

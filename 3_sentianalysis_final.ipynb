{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4609ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "from transformers import pipeline,AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c69c1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(filepath, start_year=2017, end_year=2022):\n",
    "    \"\"\"\n",
    "    读取 CSV 文件并筛选指定年份范围与行数的数据。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # 只保留需要的列\n",
    "        df = df[['Date', 'Cleaned_Tweet', 'Processed_Tweet', 'Emoji_Texts']]\n",
    "\n",
    "        # 转换日期类型\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "        # 去除无效日期\n",
    "        df = df.dropna(subset=['Date'])\n",
    "\n",
    "        # 筛选时间范围\n",
    "        df = df[(df['Date'].dt.year >= start_year) & (df['Date'].dt.year <= end_year)]\n",
    "\n",
    "        # 限制行数\n",
    "        # df = df.head(n_rows)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"验证数据完整性\"\"\"\n",
    "    required_columns = ['Date', 'Cleaned_Tweet', 'Processed_Tweet', 'Emoji_Texts']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Missing required columns\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "839cb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptive_stats(df):\n",
    "    \"\"\"\n",
    "    计算描述性统计信息\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 输入数据框\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: 包含统计信息的DataFrame（与简单版结构一致）\n",
    "    \"\"\"\n",
    "    # 保持与简单版相同的返回结构\n",
    "    stats = {\n",
    "        'total_tweets': len(df),\n",
    "        'unique_tweets': df['Processed_Tweet'].nunique(),\n",
    "        'avg_length': df['Processed_Tweet'].apply(lambda x: len(str(x).split())).mean(),\n",
    "        'emoji_count': df['Emoji_Texts'].notna().sum(),\n",
    "        'most_common_words': []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 输入验证\n",
    "        required_columns = ['Processed_Tweet', 'Emoji_Texts']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # 计算最常见单词（保持简单版逻辑）\n",
    "        all_words = ' '.join(df['Processed_Tweet'].fillna('')).split()\n",
    "        all_words = [w for w in all_words if w.isalnum()]\n",
    "        word_counts = Counter(all_words)\n",
    "        stats['most_common_words'] = word_counts.most_common(20)\n",
    "        stats['avg_tweet_length'] = df['Processed_Tweet'].str.len().mean()\n",
    "        \n",
    "        # 表情符号统计（简化版）\n",
    "        if 'Emoji_Texts' in df.columns:\n",
    "            try:\n",
    "                emoji_lists = df['Emoji_Texts'].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else x if isinstance(x, list) else []\n",
    "                )\n",
    "                stats['emoji_count'] = emoji_lists.apply(len).sum()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning in get_descriptive_stats: {str(e)}\")\n",
    "        # 保持数据结构一致，即使出错\n",
    "        stats['most_common_words'] = [('Error', 0)]\n",
    "\n",
    "    return pd.DataFrame.from_dict(stats, orient='index', columns=['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji Distribution\n",
    "def plot_top_emojis(df, emoji_column='Emoji_Texts', top_n=10, color='skyblue'):\n",
    "    \"\"\"\n",
    "    从DataFrame中统计并绘制最常用的emoji表情\n",
    "    \n",
    "    参数:\n",
    "        df (pd.DataFrame): 包含emoji数据的DataFrame\n",
    "        emoji_column (str): 包含emoji列表的列名，默认为'Emoji_Texts'\n",
    "        top_n (int): 显示前多少个emoji，默认为10\n",
    "        color (str): 条形图的颜色，默认为'skyblue'\n",
    "    \"\"\"\n",
    "    # 确保emoji列中的字符串转换为列表\n",
    "    df[emoji_column] = df[emoji_column].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    # 统计所有emoji的出现频率\n",
    "    emoji_counter = Counter(\n",
    "        [emoji for sublist in df[emoji_column] for emoji in sublist]\n",
    "    )\n",
    "    \n",
    "    # 获取出现频率最高的emoji\n",
    "    top_emoji_series = pd.Series(dict(emoji_counter.most_common(top_n)))\n",
    "    \n",
    "    # 绘制水平条形图\n",
    "    top_emoji_series.plot(\n",
    "        kind='barh', \n",
    "        title=f'Top {top_n} Emojis', \n",
    "        color=color\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Emoji')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "\n",
    "# No need to do Enhanced Sentiment Analysis Including Emojis based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef03ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tweets into financial and non-financial\n",
    "\n",
    "def classify_financial_text(text):\n",
    "    \"\"\"\n",
    "    判断文本是否包含金融相关术语\n",
    "    \n",
    "    Args:\n",
    "        text (str): 输入文本\n",
    "        \n",
    "    Returns:\n",
    "        bool: 如果文本包含金融相关术语返回True，否则返回False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 预定义金融术语正则表达式\n",
    "        financial_pattern = r'\\b(?:\\$[A-Z]{1,5}\\b|stock(?:\\s*price|s?)|price\\s*target|market\\s*(?:cap|value)|' \\\n",
    "                          r'shares|share\\s*price|earnings(?:\\s*report|per\\s*share|call)|semiconductor|chip\\s*industry|' \\\n",
    "                          r'invest(?:ing|ment)|trading|portfolio|dividend|buyback|' \\\n",
    "                          r'\\b(?:bull|bear)(?:ish|market)\\b|valuation|P/E|price-to-earnings|' \\\n",
    "                          r'analyst\\s*rating|upgrade|downgrade|(?:financial|quarterly)\\s*results|' \\\n",
    "                          r'volume|liquidity|SEC\\s*filing|10-[KQ]|IPO|FPO|secondary\\s*offering)'\n",
    "        \n",
    "        # 将输入转换为字符串并检查是否包含金融术语\n",
    "        return bool(re.search(financial_pattern, str(text), flags=re.IGNORECASE))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in classify_financial_text: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edad730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, finbert_model, bert_model):\n",
    "        self.finbert = finbert_model\n",
    "        self.bert = bert_model\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def is_initialized(self):\n",
    "        \"\"\"Check if all required models are initialized\"\"\"\n",
    "        return all([self.finbert is not None, self.bert is not None, self.vader is not None])\n",
    "\n",
    "    def analyze_financial_text(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                output = self.finbert(batch, batch_size=batch_size)\n",
    "                results.extend(output)\n",
    "            except Exception as e:\n",
    "                print(f\"FinBERT Error in batch {i}: {e}\")\n",
    "                results.extend([{'label': 'ERROR', 'score': 0}] * len(batch))\n",
    "            finally:\n",
    "                gc.collect()\n",
    "        return results\n",
    "\n",
    "    def analyze_non_financial_text(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                output = self.bert(batch,batch_size=batch_size)\n",
    "                results.extend(output)\n",
    "            except Exception as e:\n",
    "                print(f\"BERT Error in batch {i}: {e}\")\n",
    "                results.extend([{'label': 'ERROR', 'score': 0}] * len(batch))\n",
    "        return results\n",
    "\n",
    "    def analyze_vader(self, texts):\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            score = self.vader.polarity_scores(text)['compound']\n",
    "            results.append(np.clip(score / 4, -1, 1))  # standardize to [-1,1]\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def vader_to_label(score):\n",
    "        if score >= 0.05:\n",
    "            return 'positive'\n",
    "        elif score <= -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fe6d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Validating data...\n",
      "Calculating descriptive statistics...\n",
      "\n",
      "Basic Statistics:\n",
      "Total tweets: 465025\n",
      "Unique tweets: 391271\n",
      "Average length (words): 8.5\n",
      "Tweets with emojis: 95341\n",
      "\n",
      "Top 20 Most Common Words:\n",
      "nvda: 486529\n",
      "amd: 45967\n",
      "go: 41374\n",
      "buy: 34197\n",
      "today: 28284\n",
      "stock: 27585\n",
      "call: 26184\n",
      "day: 25600\n",
      "like: 25445\n",
      "get: 24343\n",
      "market: 23992\n",
      "short: 23944\n",
      "aapl: 22661\n",
      "spi: 21248\n",
      "look: 21155\n",
      "see: 20134\n",
      "week: 19522\n",
      "back: 18746\n",
      "sell: 18400\n",
      "time: 18015\n",
      "\n",
      "Average tweet length (chars): 45.8\n",
      "\n",
      "Initializing sentiment analyzers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying financial and non-financial texts...\n",
      "Financial tweets: 60990\n",
      "Non-financial tweets: 404035\n",
      "\n",
      "Performing sentiment analysis...\n",
      "Analyzing financial texts with FinBERT...\n",
      "Analyzing non-financial texts with BERT...\n",
      "Analyzing non-financial texts with VADER...\n",
      "\n",
      "Merging results...\n",
      "\n",
      "Saving results...\n",
      "Saved daily sentiment files\n",
      "\n",
      "Final Score Ranges:\n",
      "FinBERT: [-1.00, 1.00]\n",
      "BERT: [nan, nan]\n",
      "VADER: [-0.06, 0.06]\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # 1. 加载数据\n",
    "        print(\"Loading data...\")\n",
    "        df = load_and_filter_data(\"/Users/zhanghanshi/Desktop/WM_Project/my_project/cleaned_nvda.csv\")\n",
    "        if df is None or df.empty:\n",
    "            print(\"Error: Failed to load data or data is empty\")\n",
    "            return\n",
    "        \n",
    "        # 2. 验证数据\n",
    "        print(\"Validating data...\")\n",
    "        if not validate_data(df):\n",
    "            print(\"Error: Data validation failed\")\n",
    "            return\n",
    "        \n",
    "        # 3. 获取描述性统计\n",
    "        print(\"Calculating descriptive statistics...\")\n",
    "        stats_df = get_descriptive_stats(df)\n",
    "\n",
    "        # 打印统计信息\n",
    "        print(\"\\nBasic Statistics:\")\n",
    "        print(f\"Total tweets: {stats_df.loc['total_tweets', 'Value']}\")\n",
    "        print(f\"Unique tweets: {stats_df.loc['unique_tweets', 'Value']}\")\n",
    "        print(f\"Average length (words): {stats_df.loc['avg_length', 'Value']:.1f}\")\n",
    "        print(f\"Tweets with emojis: {stats_df.loc['emoji_count', 'Value']}\")\n",
    "\n",
    "        # 打印最常见单词（带防御性检查）\n",
    "        print(\"\\nTop 20 Most Common Words:\")\n",
    "        common_words = stats_df.loc['most_common_words', 'Value']\n",
    "        if isinstance(common_words, list) and len(common_words) > 0:\n",
    "            for word, count in common_words:\n",
    "                print(f\"{word}: {count}\")\n",
    "        else:\n",
    "            print(\"No common words data available\")\n",
    "\n",
    "        # 可选：打印其他你可能需要的统计信息\n",
    "        if 'avg_tweet_length' in stats_df.index:\n",
    "            print(f\"\\nAverage tweet length (chars): {stats_df.loc['avg_tweet_length', 'Value']:.1f}\")\n",
    "        \n",
    "        # 4. 初始化分析器\n",
    "        print(\"\\nInitializing sentiment analyzers...\")\n",
    "\n",
    "        # FinBERT pipeline\n",
    "        finbert_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\",truncation=True,max_length=512)\n",
    "\n",
    "        # BERT pipeline\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        bert_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer,truncation=True,max_length=512 )\n",
    "\n",
    "        # 初始化 SentimentAnalyzer 实例\n",
    "        analyzer = SentimentAnalyzer(finbert_pipeline, bert_pipeline)\n",
    "\n",
    "        if not analyzer.is_initialized():\n",
    "            print(\"Error: Failed to initialize sentiment analyzers\")\n",
    "            return\n",
    "        \n",
    "        # 5. 分类金融和非金融文本\n",
    "        print(\"\\nClassifying financial and non-financial texts...\")\n",
    "        df['is_financial'] = df['Cleaned_Tweet'].apply(classify_financial_text)\n",
    "        \n",
    "        df_fin = df[df['is_financial']].copy()\n",
    "        df_nonfin = df[~df['is_financial']].copy()\n",
    "        \n",
    "        print(f\"Financial tweets: {len(df_fin)}\")\n",
    "        print(f\"Non-financial tweets: {len(df_nonfin)}\")\n",
    "        \n",
    "        # 6. 执行情感分析\n",
    "        print(\"\\nPerforming sentiment analysis...\")\n",
    "        \n",
    "        # --- 金融文本: FinBERT ---\n",
    "        print(\"Analyzing financial texts with FinBERT...\")\n",
    "        fin_results = analyzer.analyze_financial_text(df_fin['Cleaned_Tweet'].tolist())\n",
    "        df_fin = pd.concat([\n",
    "            df_fin.reset_index(drop=True),\n",
    "            pd.DataFrame(fin_results).rename(columns={'label': 'sentiment_label'})\n",
    "        ], axis=1)\n",
    "        df_fin['sentiment_score'] = df_fin['sentiment_label'].str.lower().map(\n",
    "            {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "        ).clip(-1, 1)  # 显式标准化\n",
    "        df_fin['category'] = 'financial'\n",
    "\n",
    "        # --- 非金融文本: BERT ---\n",
    "        print(\"Analyzing non-financial texts with BERT...\")\n",
    "        bert_results = analyzer.analyze_non_financial_text(df_nonfin['Cleaned_Tweet'].tolist())\n",
    "        df_nonfin_bert = pd.concat([\n",
    "            df_nonfin.reset_index(drop=True),\n",
    "            pd.DataFrame(bert_results).rename(columns={'label': 'sentiment_label'})\n",
    "        ], axis=1)\n",
    "        df_nonfin_bert['sentiment_score'] = df_nonfin_bert['sentiment_label'].map(\n",
    "            {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "        ).clip(-1, 1)  # 显式标准化\n",
    "        df_nonfin_bert['category'] = 'non_financial'\n",
    "\n",
    "        # --- 非金融文本: VADER ---\n",
    "        print(\"Analyzing non-financial texts with VADER...\")\n",
    "        vader_scores = analyzer.analyze_vader(df_nonfin['Cleaned_Tweet'].tolist())\n",
    "        df_nonfin_vader = df_nonfin.copy()\n",
    "        df_nonfin_vader['sentiment_score'] = np.clip(vader_scores, -4, 4) / 4  # 立即标准化到[-1,1]\n",
    "        df_nonfin_vader['sentiment_label'] = [analyzer.vader_to_label(s) for s in vader_scores]\n",
    "        df_nonfin_vader['category'] = 'non_financial'\n",
    "\n",
    "        # 7. 合并结果\n",
    "        print(\"\\nMerging results...\")\n",
    "        # 获取全局日期范围\n",
    "        all_dates = pd.to_datetime(pd.concat([df_fin['Date'], df_nonfin['Date']]))\n",
    "        date_range = pd.date_range(\n",
    "            start=all_dates.min().date(), \n",
    "            end=all_dates.max().date(),\n",
    "            freq='D'\n",
    "        )\n",
    "\n",
    "        def create_daily_sentiment(df_combined, model_name):\n",
    "            \"\"\"通用每日聚合函数\"\"\"\n",
    "            df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
    "            daily_df = (\n",
    "                df_combined.groupby(df_combined['Date'].dt.date)['sentiment_score']\n",
    "                .mean()\n",
    "                .reindex(date_range)\n",
    "                .fillna(method='ffill')  # 用前值填充缺失日期\n",
    "                .reset_index()\n",
    "            )\n",
    "            daily_df.columns = ['date', 'avg_sentiment_score']\n",
    "            daily_df['model_type'] = model_name\n",
    "            return daily_df\n",
    "\n",
    "        # FinBERT + BERT\n",
    "        df_bert_combined = pd.concat([df_fin, df_nonfin_bert], ignore_index=True)\n",
    "        df_daily_bert = create_daily_sentiment(df_bert_combined, 'FinBERT+BERT')\n",
    "        \n",
    "        # FinBERT + VADER\n",
    "        df_vader_combined = pd.concat([df_fin, df_nonfin_vader], ignore_index=True)\n",
    "        df_daily_vader = create_daily_sentiment(df_vader_combined, 'FinBERT+VADER')\n",
    "\n",
    "        # 8. 保存结果\n",
    "        print(\"\\nSaving results...\")\n",
    "        df_daily_bert.to_csv(\"daily_sentiment_bert.csv\", index=False)\n",
    "        df_daily_vader.to_csv(\"daily_sentiment_vader.csv\", index=False)\n",
    "        \n",
    "        # 合并结果（可选）\n",
    "        pd.concat([df_daily_bert, df_daily_vader]).to_csv(\"all_daily_sentiment.csv\", index=False)\n",
    "        print(\"Saved daily sentiment files\")\n",
    "\n",
    "        # 9. 验证输出\n",
    "        print(\"\\nFinal Score Ranges:\")\n",
    "        print(f\"FinBERT: [{df_fin['sentiment_score'].min():.2f}, {df_fin['sentiment_score'].max():.2f}]\")\n",
    "        print(f\"BERT: [{df_nonfin_bert['sentiment_score'].min():.2f}, {df_nonfin_bert['sentiment_score'].max():.2f}]\")\n",
    "        print(f\"VADER: [{df_nonfin_vader['sentiment_score'].min():.2f}, {df_nonfin_vader['sentiment_score'].max():.2f}]\")\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc1dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbe9e71",
   "metadata": {},
   "source": [
    "# NVDA_Daily_Sentiment_Pipeline_and_Analysis\n",
    "\n",
    "- **Perform sentiment analysis on NVDA-related tweets and output daily sentiment scores.** This pipeline implements the full workflow—data loading, preprocessing, sentiment scoring, aggregation, and saving of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07557872",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 1. Data Loading & Preprocessing\n",
    "\n",
    "- Read the raw tweet dataset (e.g. `cleaned_nvda.csv`), filter to the target years (e.g. 2017–2022), and keep only the relevant columns (date, cleaned text, tokenized text, emojis, etc.).\n",
    "    \n",
    "- Conduct basic integrity checks and descriptive statistics (total tweets, unique tweets, average length, most common words, emoji usage, etc.).\n",
    "    \n",
    "\n",
    "### 2. Financial vs. Non-Financial Classification\n",
    "\n",
    "- Use regular expressions to detect finance-related terms in each tweet, splitting the dataset into “financial” and “non-financial” subsets.\n",
    "    \n",
    "\n",
    "### 3. Sentiment Analysis\n",
    "\n",
    "- **Financial tweets:** Apply FinBERT (a BERT variant fine-tuned on finance text) to produce sentiment labels (positive/neutral/negative) and confidence scores.\n",
    "    \n",
    "- **Non-financial tweets:** Apply a general-purpose BERT model to each tweet, yielding both labels and scores.\n",
    "    \n",
    "\n",
    "### 4. Aggregation & Export\n",
    "\n",
    "- Aggregate each tweet’s sentiment scores by date to compute a daily average sentiment.\n",
    "    \n",
    "- Log summary statistics and score ranges to facilitate downstream plotting and comparison with financial indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4609ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69c1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(filepath, start_year=2017, end_year=2022):\n",
    "    \"\"\"\n",
    "    Read the CSV file and filter the data to the target years and rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Only keep the relevant columns\n",
    "        df = df[['Date', 'Cleaned_Tweet', 'Processed_Tweet', 'Emoji_Texts']]\n",
    "\n",
    "        # Convert date type\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "        # Remove invalid dates\n",
    "        df = df.dropna(subset=['Date'])\n",
    "\n",
    "        # Filter time range\n",
    "        df = df[(df['Date'].dt.year >= start_year) & (df['Date'].dt.year <= end_year)]\n",
    "\n",
    "        # Limit rows\n",
    "        # df = df.head(n_rows)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"Verify data integrity\"\"\"\n",
    "    required_columns = ['Date', 'Cleaned_Tweet', 'Processed_Tweet', 'Emoji_Texts']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Missing required columns\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839cb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptive_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate descriptive statistics\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input Dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with statistics (same structure as simple version)\n",
    "    \"\"\"\n",
    "    # Keep the same return structure as simple version\n",
    "    stats = {\n",
    "        'total_tweets': len(df),\n",
    "        'unique_tweets': df['Processed_Tweet'].nunique(),\n",
    "        'avg_length': df['Processed_Tweet'].apply(lambda x: len(str(x).split())).mean(),\n",
    "        'emoji_count': df['Emoji_Texts'].notna().sum(),\n",
    "        'most_common_words': []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Input validation\n",
    "        required_columns = ['Processed_Tweet', 'Emoji_Texts']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # Counting the most common words\n",
    "        all_words = ' '.join(df['Processed_Tweet'].fillna('')).split()\n",
    "        all_words = [w for w in all_words if w.isalnum()]\n",
    "        word_counts = Counter(all_words)\n",
    "        stats['most_common_words'] = word_counts.most_common(20)\n",
    "        stats['avg_tweet_length'] = df['Processed_Tweet'].str.len().mean()\n",
    "        \n",
    "        # Emoji statistics\n",
    "        if 'Emoji_Texts' in df.columns:\n",
    "            try:\n",
    "                emoji_lists = df['Emoji_Texts'].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else x if isinstance(x, list) else []\n",
    "                )\n",
    "                stats['emoji_count'] = emoji_lists.apply(len).sum()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning in get_descriptive_stats: {str(e)}\")\n",
    "        # Keep the data structure consistent even if there is an error\n",
    "        stats['most_common_words'] = [('Error', 0)]\n",
    "\n",
    "    return pd.DataFrame.from_dict(stats, orient='index', columns=['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji Distribution\n",
    "def plot_top_emojis(df, emoji_column='Emoji_Texts', top_n=10, color='skyblue'):\n",
    "    \"\"\"\n",
    "    Count and plot the most common emojis from a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing emoji data\n",
    "        emoji_column (str): Column name containing emoji lists, default is 'Emoji_Texts'\n",
    "        top_n (int): Number of emojis to display, default is 10\n",
    "        color (str): Bar color, default is 'skyblue'\n",
    "    \"\"\"\n",
    "    # Ensure the strings in the emoji column are converted to lists\n",
    "    df[emoji_column] = df[emoji_column].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    # Count all emoji occurrences\n",
    "    emoji_counter = Counter(\n",
    "        [emoji for sublist in df[emoji_column] for emoji in sublist]\n",
    "    )\n",
    "    \n",
    "    # Get the most frequent emoji\n",
    "    top_emoji_series = pd.Series(dict(emoji_counter.most_common(top_n)))\n",
    "    \n",
    "    # Plot horizontal bar chart\n",
    "    top_emoji_series.plot(\n",
    "        kind='barh', \n",
    "        title=f'Top {top_n} Emojis', \n",
    "        color=color\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Emoji')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "\n",
    "# No need to do Enhanced Sentiment Analysis Including Emojis based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef03ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tweets into financial and non-financial\n",
    "\n",
    "def classify_financial_text(text):\n",
    "    \"\"\"\n",
    "    Determine if text contains financial terms\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        bool: If text contains financial terms, return True, otherwise return False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Predefined financial terms regex\n",
    "        financial_pattern = r'\\b(?:\\$[A-Z]{1,5}\\b|stock(?:\\s*price|s?)|price\\s*target|market\\s*(?:cap|value)|' \\\n",
    "                          r'shares|share\\s*price|earnings(?:\\s*report|per\\s*share|call)|semiconductor|chip\\s*industry|' \\\n",
    "                          r'invest(?:ing|ment)|trading|portfolio|dividend|buyback|' \\\n",
    "                          r'\\b(?:bull|bear)(?:ish|market)\\b|valuation|P/E|price-to-earnings|' \\\n",
    "                          r'analyst\\s*rating|upgrade|downgrade|(?:financial|quarterly)\\s*results|' \\\n",
    "                          r'volume|liquidity|SEC\\s*filing|10-[KQ]|IPO|FPO|secondary\\s*offering)'\n",
    "        \n",
    "        # Convert input to string and check if it contains financial terms\n",
    "        return bool(re.search(financial_pattern, str(text), flags=re.IGNORECASE))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in classify_financial_text: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8c4b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edad730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, finbert_model, roberta_model):\n",
    "        self.finbert = finbert_model\n",
    "        self.roberta = roberta_model\n",
    "\n",
    "    def is_initialized(self):\n",
    "        \"\"\"Check if all required models are initialized\"\"\"\n",
    "        return all([self.finbert is not None, self.roberta is not None])\n",
    "\n",
    "    def analyze_financial_text(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                output = self.finbert(batch, batch_size=batch_size)\n",
    "                results.extend(output)\n",
    "            except Exception as e:\n",
    "                print(f\"FinBERT Error in batch {i}: {e}\")\n",
    "                results.extend([{'label': 'ERROR', 'score': 0}] * len(batch))\n",
    "            finally:\n",
    "                gc.collect()\n",
    "        return results\n",
    "\n",
    "    def analyze_non_financial_text(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                output = self.roberta(batch, batch_size=batch_size)\n",
    "                results.extend(output)\n",
    "            except Exception as e:\n",
    "                print(f\"RoBERTa Error in batch {i}: {e}\")\n",
    "                results.extend([{'label': 'ERROR', 'score': 0}] * len(batch))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe6d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Error loading data: [Errno 2] No such file or directory: '../2_data/cleaned_nvda.csv'\n",
      "Error: Failed to load data or data is empty\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas() \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 1. Load data\n",
    "        print(\"Loading data...\")\n",
    "        df = load_and_filter_data(\"../2_data/cleaned_nvda.csv\")\n",
    "        if df is None or df.empty:\n",
    "            print(\"Error: Failed to load data or data is empty\")\n",
    "            return\n",
    "        \n",
    "        # 2. Validate data\n",
    "        print(\"Validating data...\")\n",
    "        if not validate_data(df):\n",
    "            print(\"Error: Data validation failed\")\n",
    "            return\n",
    "        \n",
    "        # 3. Get descriptive statistics\n",
    "        print(\"Calculating descriptive statistics...\")\n",
    "        stats_df = get_descriptive_stats(df)\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"\\nBasic Statistics:\")\n",
    "        print(f\"Total tweets: {stats_df.loc['total_tweets', 'Value']}\")\n",
    "        print(f\"Unique tweets: {stats_df.loc['unique_tweets', 'Value']}\")\n",
    "        print(f\"Average length (words): {stats_df.loc['avg_length', 'Value']:.1f}\")\n",
    "        print(f\"Tweets with emojis: {stats_df.loc['emoji_count', 'Value']}\")\n",
    "\n",
    "        # Print common words\n",
    "        print(\"\\nTop 20 Most Common Words:\")\n",
    "        common_words = stats_df.loc['most_common_words', 'Value']\n",
    "        if isinstance(common_words, list) and len(common_words) > 0:\n",
    "            for word, count in common_words:\n",
    "                print(f\"{word}: {count}\")\n",
    "        else:\n",
    "            print(\"No common words data available\")\n",
    "\n",
    "        if 'avg_tweet_length' in stats_df.index:\n",
    "            print(f\"\\nAverage tweet length (chars): {stats_df.loc['avg_tweet_length', 'Value']:.1f}\")\n",
    "        \n",
    "        # 4. Initialize sentiment analyzers\n",
    "        print(\"\\nInitializing sentiment analyzers...\")\n",
    "\n",
    "        # FinBERT pipeline for financial tweets\n",
    "        finbert_pipeline = pipeline(\"sentiment-analysis\", \n",
    "                                  model=\"yiyanghkust/finbert-tone\",\n",
    "                                  truncation=True,\n",
    "                                  max_length=512)\n",
    "\n",
    "        # RoBERTa pipeline for non-financial tweets\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "        roberta_pipeline = pipeline(\"sentiment-analysis\", \n",
    "                                  model=model, \n",
    "                                  tokenizer=tokenizer, \n",
    "                                  truncation=True, \n",
    "                                  max_length=512)\n",
    "\n",
    "        # Initialize SentimentAnalyzer instance\n",
    "        analyzer = SentimentAnalyzer(finbert_pipeline, roberta_pipeline)\n",
    "\n",
    "        if not analyzer.is_initialized():\n",
    "            print(\"Error: Failed to initialize sentiment analyzers\")\n",
    "            return\n",
    "        \n",
    "        # 5. Classify financial and non-financial texts\n",
    "        print(\"\\nClassifying financial and non-financial texts...\")\n",
    "        df['is_financial'] = df['Cleaned_Tweet'].apply(classify_financial_text)\n",
    "        \n",
    "        df_fin = df[df['is_financial']].copy()\n",
    "        df_nonfin = df[~df['is_financial']].copy()\n",
    "        \n",
    "        print(f\"Financial tweets: {len(df_fin)}\")\n",
    "        print(f\"Non-financial tweets: {len(df_nonfin)}\")\n",
    "        \n",
    "        # 6. Perform sentiment analysis\n",
    "        print(\"\\nPerforming sentiment analysis...\")\n",
    "        \n",
    "        # Financial texts: FinBERT\n",
    "        print(\"Analyzing financial texts with FinBERT...\")\n",
    "        fin_texts = df_fin['Cleaned_Tweet'].tolist()\n",
    "        fin_results = []\n",
    "        for text in tqdm(fin_texts, desc=\"FinBERT Sentiment\", ncols=80):\n",
    "            fin_results.append(analyzer.analyze_financial_text([text])[0]) \n",
    "        df_fin = pd.concat([\n",
    "            df_fin.reset_index(drop=True),\n",
    "            pd.DataFrame(fin_results).rename(columns={'label': 'sentiment_label'})\n",
    "        ], axis=1)\n",
    "\n",
    "        # Map FinBERT scores\n",
    "        df_fin['sentiment_score'] = df_fin['sentiment_label'].str.lower().map(\n",
    "            {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "        ).clip(-1, 1)\n",
    "        df_fin['category'] = 'financial'\n",
    "\n",
    "        # Non-financial texts: RoBERTa\n",
    "        print(\"Analyzing non-financial texts with RoBERTa...\")\n",
    "        nonfin_texts = df_nonfin['Cleaned_Tweet'].tolist()\n",
    "        roberta_results = []        \n",
    "        for text in tqdm(nonfin_texts, desc=\"RoBERTa Sentiment\", ncols=80):\n",
    "            roberta_results.append(analyzer.analyze_non_financial_text([text])[0])\n",
    "        df_nonfin = pd.concat([\n",
    "            df_nonfin.reset_index(drop=True),\n",
    "            pd.DataFrame(roberta_results).rename(columns={'label': 'sentiment_label'})\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Map RoBERTa scores (POSITIVE, NEUTRAL, NEGATIVE)\n",
    "        # Map RoBERTa sentiment labels to numeric score like FinBERT\n",
    "        roberta_label_map = {\n",
    "            'LABEL_0': -1,  # negative\n",
    "            'LABEL_1':  0,  # neutral\n",
    "            'LABEL_2':  1   # positive\n",
    "        }\n",
    "        df_nonfin['sentiment_score'] = df_nonfin['sentiment_label'].map(roberta_label_map).astype(float)\n",
    "        df_nonfin['category'] = 'non_financial'\n",
    "\n",
    "        # 7. Merge results\n",
    "        print(\"\\nMerging results...\")\n",
    "        # Get the global date range\n",
    "        all_dates = pd.to_datetime(pd.concat([df_fin['Date'], df_nonfin['Date']]))\n",
    "        date_range = pd.date_range(\n",
    "            start=all_dates.min().date(), \n",
    "            end=all_dates.max().date(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        # Merge all sentiment results\n",
    "        df_tweet_sentiment = pd.concat([df_fin, df_nonfin], ignore_index=True)\n",
    "\n",
    "        # Save tweet-level sentiments\n",
    "        cols_to_save = ['Date', 'Cleaned_Tweet', 'sentiment_score', 'sentiment_label', 'category']\n",
    "        for col in ['created_at', 'username', 'likes']:\n",
    "            if col in df_tweet_sentiment.columns and col not in cols_to_save:\n",
    "                cols_to_save.append(col)\n",
    "\n",
    "        df_tweet_sentiment[cols_to_save].to_csv(\"../2_data/tweet_level_sentiment.csv\", index=False)\n",
    "        print(\"Saved tweet-level sentiment file: tweet_level_sentiment.csv\")\n",
    "\n",
    "        def create_daily_sentiment(df_combined, model_name):\n",
    "            \"\"\"General daily aggregation function\"\"\"\n",
    "            df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
    "            daily_df = (\n",
    "                df_combined.groupby(df_combined['Date'].dt.date)['sentiment_score']\n",
    "                .agg(['mean', 'count', 'std'])\n",
    "                .reindex(date_range)\n",
    "                .fillna(method='ffill')\n",
    "                .reset_index()\n",
    "            )\n",
    "            daily_df.columns = ['date', 'avg_sentiment_score', 'tweet_count', 'sentiment_std']\n",
    "            daily_df['model_type'] = model_name\n",
    "            return daily_df\n",
    "\n",
    "        # Create daily sentiment DataFrames\n",
    "        print(\"\\nCreating daily sentiment aggregates...\")\n",
    "        df_daily_fin = create_daily_sentiment(df_fin, 'FinBERT')\n",
    "        df_daily_roberta = create_daily_sentiment(df_nonfin, 'RoBERTa')\n",
    "        \n",
    "        # Create combined daily sentiment\n",
    "        df_combined = pd.concat([df_fin, df_nonfin], ignore_index=True)\n",
    "        df_daily_combined = create_daily_sentiment(df_combined, 'FinBERT+RoBERTa')\n",
    "\n",
    "        # 8. Save results\n",
    "        print(\"\\nSaving results...\")\n",
    "        df_daily_combined.to_csv(\"../2_data/daily_sentiment_combined.csv\", index=False)\n",
    "        pd.concat([df_daily_fin, df_daily_roberta]).to_csv(\"../2_data/all_daily_sentiment.csv\", index=False)\n",
    "        print(\"Saved daily sentiment files\")\n",
    "\n",
    "        # 9. Validate output\n",
    "        print(\"\\nFinal Score Ranges:\")\n",
    "        print(f\"FinBERT: [{df_fin['sentiment_score'].min():.2f}, {df_fin['sentiment_score'].max():.2f}]\")\n",
    "        print(f\"RoBERTa: [{df_nonfin['sentiment_score'].min():.2f}, {df_nonfin['sentiment_score'].max():.2f}]\")\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b432be4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "accc1dca",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "### (1) Data Volume & Basic Characteristics\n",
    "\n",
    "- **Total tweets:** 465,025\n",
    "    \n",
    "- **Unique tweets:** 391,275 (indicating some duplicates/retweets)\n",
    "    \n",
    "- **Average tweet length:** 8.5 words (45.8 characters)\n",
    "    \n",
    "- **Tweets containing emoji:** 95,341\n",
    "    \n",
    "- **Most common words:** “nvda”, “amd”, “buy”, “stock”, “market”, etc., reflecting a strong focus on equities and financial topics\n",
    "    \n",
    "\n",
    "### (2) Financial vs. Non-Financial Tweet Classification\n",
    "\n",
    "- **Financial tweets:** 60,990 (≈ 13%)\n",
    "    \n",
    "- **Non-financial tweets:** 404,035 (≈ 87%)\n",
    "    \n",
    "- Indicates that although most tweets mention NVDA, only a small fraction employ explicit financial terminology or express investment viewpoints.\n",
    "    \n",
    "\n",
    "### (3) Sentiment Analysis Results\n",
    "\n",
    "- **Financial tweets (FinBERT):** sentiment scores in the range [–1, 1], showing a reasonable distribution\n",
    "    \n",
    "- **Non-financial tweets (RoBERTa):** sentiment scores in the range [–1, 1], showing a reasonable distribution\n",
    "    \n",
    "- **Daily sentiment series:** aggregated as daily averages to facilitate alignment with market data in downstream analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

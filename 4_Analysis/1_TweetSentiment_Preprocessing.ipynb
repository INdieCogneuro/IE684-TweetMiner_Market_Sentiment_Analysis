{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbe9e71",
   "metadata": {},
   "source": [
    "# NVDA_Daily_Sentiment_Pipeline_and_Analysis\n",
    "\n",
    "- **Perform sentiment analysis on NVDA-related tweets and output daily sentiment scores.** This pipeline implements the full workflow—data loading, preprocessing, sentiment scoring, aggregation, and saving of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07557872",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "### 1. Data Loading & Preprocessing\n",
    "\n",
    "- Read the raw tweet dataset (e.g. `cleaned_nvda.csv`), filter to the target years (e.g. 2017–2022), and keep only the relevant columns (date, cleaned text, tokenized text, emojis, etc.).\n",
    "    \n",
    "- Conduct basic integrity checks and descriptive statistics (total tweets, unique tweets, average length, most common words, emoji usage, etc.).\n",
    "    \n",
    "\n",
    "### 2. Financial vs. Non-Financial Classification\n",
    "\n",
    "- Use regular expressions to detect finance-related terms in each tweet, splitting the dataset into “financial” and “non-financial” subsets.\n",
    "    \n",
    "\n",
    "### 3. Sentiment Analysis\n",
    "\n",
    "- **Financial tweets:** Apply FinBERT (a BERT variant fine-tuned on finance text) to produce sentiment labels (positive/neutral/negative) and confidence scores.\n",
    "    \n",
    "- **Non-financial tweets:** Apply a general-purpose BERT model and VADER (a rule-based sentiment analyzer) to each tweet, yielding both labels and scores.\n",
    "    \n",
    "\n",
    "### 4. Aggregation & Export\n",
    "\n",
    "- Aggregate each tweet’s sentiment scores by date to compute a daily average sentiment.\n",
    "    \n",
    "- Save two versions of the daily sentiment series—FinBERT + BERT and FinBERT + VADER—to separate CSVs (e.g. `daily_sentiment_bert.csv` and `daily_sentiment_vader.csv`).\n",
    "    \n",
    "- Log summary statistics and score ranges to facilitate downstream plotting and comparison with financial indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4609ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import torch\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "from transformers import pipeline,AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69c1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_data(filepath, start_year=2017, end_year=2022):\n",
    "    \"\"\"\n",
    "    Read the CSV file and filter the data to the target years and rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Only keep the relevant columns\n",
    "        df = df[['Date', 'Cleaned_Tweet', 'Processed_Tweet', 'Emoji_Texts']]\n",
    "\n",
    "        # Convert date type\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "        # Remove invalid dates\n",
    "        df = df.dropna(subset=['Date'])\n",
    "\n",
    "        # Filter time range\n",
    "        df = df[(df['Date'].dt.year >= start_year) & (df['Date'].dt.year <= end_year)]\n",
    "\n",
    "        # Limit rows\n",
    "        # df = df.head(n_rows)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"Verify data integrity\"\"\"\n",
    "    required_columns = ['Date', 'Cleaned_Tweet', 'Processed_Tweet', 'Emoji_Texts']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Missing required columns\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839cb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptive_stats(df):\n",
    "    \"\"\"\n",
    "    Calculate descriptive statistics\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input Dataframe\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with statistics (same structure as simple version)\n",
    "    \"\"\"\n",
    "    # Keep the same return structure as simple version\n",
    "    stats = {\n",
    "        'total_tweets': len(df),\n",
    "        'unique_tweets': df['Processed_Tweet'].nunique(),\n",
    "        'avg_length': df['Processed_Tweet'].apply(lambda x: len(str(x).split())).mean(),\n",
    "        'emoji_count': df['Emoji_Texts'].notna().sum(),\n",
    "        'most_common_words': []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Input validation\n",
    "        required_columns = ['Processed_Tweet', 'Emoji_Texts']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # Counting the most common words\n",
    "        all_words = ' '.join(df['Processed_Tweet'].fillna('')).split()\n",
    "        all_words = [w for w in all_words if w.isalnum()]\n",
    "        word_counts = Counter(all_words)\n",
    "        stats['most_common_words'] = word_counts.most_common(20)\n",
    "        stats['avg_tweet_length'] = df['Processed_Tweet'].str.len().mean()\n",
    "        \n",
    "        # Emoji statistics\n",
    "        if 'Emoji_Texts' in df.columns:\n",
    "            try:\n",
    "                emoji_lists = df['Emoji_Texts'].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else x if isinstance(x, list) else []\n",
    "                )\n",
    "                stats['emoji_count'] = emoji_lists.apply(len).sum()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Warning in get_descriptive_stats: {str(e)}\")\n",
    "        # Keep the data structure consistent even if there is an error\n",
    "        stats['most_common_words'] = [('Error', 0)]\n",
    "\n",
    "    return pd.DataFrame.from_dict(stats, orient='index', columns=['Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji Distribution\n",
    "def plot_top_emojis(df, emoji_column='Emoji_Texts', top_n=10, color='skyblue'):\n",
    "    \"\"\"\n",
    "    Count and plot the most common emojis from a DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing emoji data\n",
    "        emoji_column (str): Column name containing emoji lists, default is 'Emoji_Texts'\n",
    "        top_n (int): Number of emojis to display, default is 10\n",
    "        color (str): Bar color, default is 'skyblue'\n",
    "    \"\"\"\n",
    "    # Ensure the strings in the emoji column are converted to lists\n",
    "    df[emoji_column] = df[emoji_column].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    # Count all emoji occurrences\n",
    "    emoji_counter = Counter(\n",
    "        [emoji for sublist in df[emoji_column] for emoji in sublist]\n",
    "    )\n",
    "    \n",
    "    # Get the most frequent emoji\n",
    "    top_emoji_series = pd.Series(dict(emoji_counter.most_common(top_n)))\n",
    "    \n",
    "    # Plot horizontal bar chart\n",
    "    top_emoji_series.plot(\n",
    "        kind='barh', \n",
    "        title=f'Top {top_n} Emojis', \n",
    "        color=color\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Emoji')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    " \n",
    "\n",
    "# No need to do Enhanced Sentiment Analysis Including Emojis based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef03ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tweets into financial and non-financial\n",
    "\n",
    "def classify_financial_text(text):\n",
    "    \"\"\"\n",
    "    Determine if text contains financial terms\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        bool: If text contains financial terms, return True, otherwise return False\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Predefined financial terms regex\n",
    "        financial_pattern = r'\\b(?:\\$[A-Z]{1,5}\\b|stock(?:\\s*price|s?)|price\\s*target|market\\s*(?:cap|value)|' \\\n",
    "                          r'shares|share\\s*price|earnings(?:\\s*report|per\\s*share|call)|semiconductor|chip\\s*industry|' \\\n",
    "                          r'invest(?:ing|ment)|trading|portfolio|dividend|buyback|' \\\n",
    "                          r'\\b(?:bull|bear)(?:ish|market)\\b|valuation|P/E|price-to-earnings|' \\\n",
    "                          r'analyst\\s*rating|upgrade|downgrade|(?:financial|quarterly)\\s*results|' \\\n",
    "                          r'volume|liquidity|SEC\\s*filing|10-[KQ]|IPO|FPO|secondary\\s*offering)'\n",
    "        \n",
    "        # Convert input to string and check if it contains financial terms\n",
    "        return bool(re.search(financial_pattern, str(text), flags=re.IGNORECASE))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in classify_financial_text: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edad730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(self, finbert_model, bert_model):\n",
    "        self.finbert = finbert_model\n",
    "        self.bert = bert_model\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def is_initialized(self):\n",
    "        \"\"\"Check if all required models are initialized\"\"\"\n",
    "        return all([self.finbert is not None, self.bert is not None, self.vader is not None])\n",
    "\n",
    "    def analyze_financial_text(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                output = self.finbert(batch, batch_size=batch_size)\n",
    "                results.extend(output)\n",
    "            except Exception as e:\n",
    "                print(f\"FinBERT Error in batch {i}: {e}\")\n",
    "                results.extend([{'label': 'ERROR', 'score': 0}] * len(batch))\n",
    "            finally:\n",
    "                gc.collect()\n",
    "        return results\n",
    "\n",
    "    def analyze_non_financial_text(self, texts, batch_size=32):\n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            try:\n",
    "                output = self.bert(batch,batch_size=batch_size)\n",
    "                results.extend(output)\n",
    "            except Exception as e:\n",
    "                print(f\"BERT Error in batch {i}: {e}\")\n",
    "                results.extend([{'label': 'ERROR', 'score': 0}] * len(batch))\n",
    "        return results\n",
    "\n",
    "    def analyze_vader(self, texts):\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            score = self.vader.polarity_scores(text)['compound']\n",
    "            results.append(np.clip(score / 4, -1, 1))  # standardize to [-1,1]\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def vader_to_label(score):\n",
    "        if score >= 0.05:\n",
    "            return 'positive'\n",
    "        elif score <= -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe6d9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Validating data...\n",
      "Calculating descriptive statistics...\n",
      "\n",
      "Basic Statistics:\n",
      "Total tweets: 465025\n",
      "Unique tweets: 391275\n",
      "Average length (words): 8.5\n",
      "Tweets with emojis: 95341\n",
      "\n",
      "Top 20 Most Common Words:\n",
      "nvda: 486529\n",
      "amd: 45968\n",
      "go: 41375\n",
      "buy: 34198\n",
      "today: 28285\n",
      "stock: 27587\n",
      "call: 26186\n",
      "day: 25600\n",
      "like: 25445\n",
      "get: 24343\n",
      "market: 23992\n",
      "short: 23944\n",
      "aapl: 22661\n",
      "spi: 21248\n",
      "look: 21155\n",
      "see: 20134\n",
      "week: 19523\n",
      "back: 18747\n",
      "sell: 18401\n",
      "time: 18015\n",
      "\n",
      "Average tweet length (chars): 45.8\n",
      "\n",
      "Initializing sentiment analyzers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying financial and non-financial texts...\n",
      "Financial tweets: 60990\n",
      "Non-financial tweets: 404035\n",
      "\n",
      "Performing sentiment analysis...\n",
      "Analyzing financial texts with FinBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FinBERT Sentiment: 100%|████████████████| 60990/60990 [3:35:28<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing non-financial texts with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT Sentiment: 100%|█████████████████| 404035/404035 [1:45:30<00:00, 63.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing non-financial texts with VADER...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VADER Sentiment: 100%|████████████| 404035/404035 [00:00<00:00, 12384591.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging results...\n",
      "Saved tweet-level sentiment file: tweet_level_sentiment.csv\n",
      "\n",
      "Saving results...\n",
      "Saved daily sentiment files\n",
      "\n",
      "Final Score Ranges:\n",
      "FinBERT: [-1.00, 1.00]\n",
      "BERT: [nan, nan]\n",
      "VADER: [-0.06, 0.06]\n",
      "\n",
      "Analysis completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/lc8n68_d7ld6yymqc_lsnrr00000gn/T/ipykernel_38248/10632280.py:148: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  .fillna(method='ffill')  # use previous value to fill missing dates\n",
      "/var/folders/rn/lc8n68_d7ld6yymqc_lsnrr00000gn/T/ipykernel_38248/10632280.py:148: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  .fillna(method='ffill')  # use previous value to fill missing dates\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas() \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 1. Laod data\n",
    "        print(\"Loading data...\")\n",
    "        df = load_and_filter_data(\"../2_data/cleaned_nvda.csv\")\n",
    "        if df is None or df.empty:\n",
    "            print(\"Error: Failed to load data or data is empty\")\n",
    "            return\n",
    "        \n",
    "        # 2. Validate data\n",
    "        print(\"Validating data...\")\n",
    "        if not validate_data(df):\n",
    "            print(\"Error: Data validation failed\")\n",
    "            return\n",
    "        \n",
    "        # 3. Get descriptive statistics\n",
    "        print(\"Calculating descriptive statistics...\")\n",
    "        stats_df = get_descriptive_stats(df)\n",
    "\n",
    "        # Print\n",
    "        print(\"\\nBasic Statistics:\")\n",
    "        print(f\"Total tweets: {stats_df.loc['total_tweets', 'Value']}\")\n",
    "        print(f\"Unique tweets: {stats_df.loc['unique_tweets', 'Value']}\")\n",
    "        print(f\"Average length (words): {stats_df.loc['avg_length', 'Value']:.1f}\")\n",
    "        print(f\"Tweets with emojis: {stats_df.loc['emoji_count', 'Value']}\")\n",
    "\n",
    "        # Print the most common words (with defensive checks)\n",
    "        print(\"\\nTop 20 Most Common Words:\")\n",
    "        common_words = stats_df.loc['most_common_words', 'Value']\n",
    "        if isinstance(common_words, list) and len(common_words) > 0:\n",
    "            for word, count in common_words:\n",
    "                print(f\"{word}: {count}\")\n",
    "        else:\n",
    "            print(\"No common words data available\")\n",
    "\n",
    "        # Optional: Print other statistics you might need\n",
    "        if 'avg_tweet_length' in stats_df.index:\n",
    "            print(f\"\\nAverage tweet length (chars): {stats_df.loc['avg_tweet_length', 'Value']:.1f}\")\n",
    "        \n",
    "        # 4. Initialize sentiment analyzers\n",
    "        print(\"\\nInitializing sentiment analyzers...\")\n",
    "\n",
    "        # FinBERT pipeline\n",
    "        finbert_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\",truncation=True,max_length=512)\n",
    "\n",
    "        # BERT pipeline\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        bert_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer,truncation=True,max_length=512 )\n",
    "\n",
    "        # Initialize SentimentAnalyzer instance\n",
    "        analyzer = SentimentAnalyzer(finbert_pipeline, bert_pipeline)\n",
    "\n",
    "        if not analyzer.is_initialized():\n",
    "            print(\"Error: Failed to initialize sentiment analyzers\")\n",
    "            return\n",
    "        \n",
    "        # 5. Classify financial and non-financial texts\n",
    "        print(\"\\nClassifying financial and non-financial texts...\")\n",
    "        df['is_financial'] = df['Cleaned_Tweet'].apply(classify_financial_text)\n",
    "        \n",
    "        df_fin = df[df['is_financial']].copy()\n",
    "        df_nonfin = df[~df['is_financial']].copy()\n",
    "        \n",
    "        print(f\"Financial tweets: {len(df_fin)}\")\n",
    "        print(f\"Non-financial tweets: {len(df_nonfin)}\")\n",
    "        \n",
    "        # 6. Perform sentiment analysis\n",
    "        print(\"\\nPerforming sentiment analysis...\")\n",
    "        \n",
    "        # --- Financial texts: FinBERT ---\n",
    "        print(\"Analyzing financial texts with FinBERT...\")\n",
    "        \n",
    "        fin_texts = df_fin['Cleaned_Tweet'].tolist()\n",
    "        fin_results = []\n",
    "        for text in tqdm(fin_texts, desc=\"FinBERT Sentiment\", ncols=80):\n",
    "            fin_results.append(analyzer.analyze_financial_text([text])[0]) \n",
    "        df_fin = pd.concat([\n",
    "            df_fin.reset_index(drop=True),\n",
    "            pd.DataFrame(fin_results).rename(columns={'label': 'sentiment_label'})\n",
    "        ], axis=1)\n",
    "\n",
    "        df_fin['sentiment_score'] = df_fin['sentiment_label'].str.lower().map(\n",
    "            {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "        ).clip(-1, 1)  # explicitly standardize\n",
    "        df_fin['category'] = 'financial'\n",
    "\n",
    "        # --- Non-financial texts: BERT ---\n",
    "        print(\"Analyzing non-financial texts with BERT...\")\n",
    "        bert_texts = df_nonfin['Cleaned_Tweet'].tolist()\n",
    "        bert_results = []        \n",
    "        for text in tqdm(bert_texts, desc=\"BERT Sentiment\", ncols=80):\n",
    "            bert_results.append(analyzer.analyze_non_financial_text([text])[0])\n",
    "        df_nonfin_bert = pd.concat([\n",
    "            df_nonfin.reset_index(drop=True),\n",
    "            pd.DataFrame(bert_results).rename(columns={'label': 'sentiment_label'})\n",
    "        ], axis=1)\n",
    "        df_nonfin_bert['sentiment_score'] = df_nonfin_bert['sentiment_label'].map(\n",
    "            {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "        ).clip(-1, 1)  # explicitly standardize\n",
    "        df_nonfin_bert['category'] = 'non_financial'\n",
    "\n",
    "        # --- Non-financial texts: VADER ---\n",
    "        print(\"Analyzing non-financial texts with VADER...\")\n",
    "        vader_scores = list(tqdm(analyzer.analyze_vader(df_nonfin['Cleaned_Tweet'].tolist()), desc=\"VADER Sentiment\", ncols=80))\n",
    "\n",
    "        df_nonfin_vader = df_nonfin.copy()\n",
    "        df_nonfin_vader['sentiment_score'] = np.clip(vader_scores, -4, 4) / 4  # 立即标准化到[-1,1]\n",
    "        df_nonfin_vader['sentiment_label'] = [analyzer.vader_to_label(s) for s in vader_scores]\n",
    "        df_nonfin_vader['category'] = 'non_financial'\n",
    "\n",
    "        # 7. Merge results\n",
    "        print(\"\\nMerging results...\")\n",
    "        # Get the global date range\n",
    "        all_dates = pd.to_datetime(pd.concat([df_fin['Date'], df_nonfin['Date']]))\n",
    "        date_range = pd.date_range(\n",
    "            start=all_dates.min().date(), \n",
    "            end=all_dates.max().date(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        # merge all the sentiment results\n",
    "        df_tweet_sentiment = pd.concat([df_fin, df_nonfin_bert, df_nonfin_vader], ignore_index=True)\n",
    "\n",
    "        # only keep the columns that are needed\n",
    "        cols_to_save = ['Date', 'Cleaned_Tweet', 'sentiment_score', 'sentiment_label', 'category']\n",
    "        for col in ['created_at', 'username', 'likes']:\n",
    "            if col in df_tweet_sentiment.columns and col not in cols_to_save:\n",
    "                cols_to_save.append(col)\n",
    "\n",
    "        df_tweet_sentiment[cols_to_save].to_csv(\"../2_data/tweet_level_sentiment.csv\", index=False)\n",
    "        print(\"Saved tweet-level sentiment file: tweet_level_sentiment.csv\")\n",
    "\n",
    "\n",
    "\n",
    "        def create_daily_sentiment(df_combined, model_name):\n",
    "            \"\"\"General daily aggregation function\"\"\"\n",
    "            df_combined['Date'] = pd.to_datetime(df_combined['Date'])\n",
    "            daily_df = (\n",
    "                df_combined.groupby(df_combined['Date'].dt.date)['sentiment_score']\n",
    "                .mean()\n",
    "                .reindex(date_range)\n",
    "                .fillna(method='ffill')  # use previous value to fill missing dates\n",
    "                .reset_index()\n",
    "            )\n",
    "            daily_df.columns = ['date', 'avg_sentiment_score']\n",
    "            daily_df['model_type'] = model_name\n",
    "            return daily_df\n",
    "\n",
    "        # FinBERT + BERT\n",
    "        df_bert_combined = pd.concat([df_fin, df_nonfin_bert], ignore_index=True)\n",
    "        df_daily_bert = create_daily_sentiment(df_bert_combined, 'FinBERT+BERT')\n",
    "        \n",
    "        # FinBERT + VADER\n",
    "        df_vader_combined = pd.concat([df_fin, df_nonfin_vader], ignore_index=True)\n",
    "        df_daily_vader = create_daily_sentiment(df_vader_combined, 'FinBERT+VADER')\n",
    "\n",
    "        # 8. Save results\n",
    "        print(\"\\nSaving results...\")\n",
    "        df_daily_bert.to_csv(\"../2_data/daily_sentiment_bert.csv\", index=False)\n",
    "        df_daily_vader.to_csv(\"../2_data/daily_sentiment_vader.csv\", index=False)\n",
    "        \n",
    "        # Merge results (optional)\n",
    "        pd.concat([df_daily_bert, df_daily_vader]).to_csv(\"../2_data/all_daily_sentiment.csv\", index=False)\n",
    "        print(\"Saved daily sentiment files\")\n",
    "\n",
    "        # 9. Validate output\n",
    "        print(\"\\nFinal Score Ranges:\")\n",
    "        print(f\"FinBERT: [{df_fin['sentiment_score'].min():.2f}, {df_fin['sentiment_score'].max():.2f}]\")\n",
    "        print(f\"BERT: [{df_nonfin_bert['sentiment_score'].min():.2f}, {df_nonfin_bert['sentiment_score'].max():.2f}]\")\n",
    "        print(f\"VADER: [{df_nonfin_vader['sentiment_score'].min():.2f}, {df_nonfin_vader['sentiment_score'].max():.2f}]\")\n",
    "\n",
    "        print(\"\\nAnalysis completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc1dca",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "\n",
    "### (1) Data Volume & Basic Characteristics\n",
    "\n",
    "- **Total tweets:** 465,025\n",
    "    \n",
    "- **Unique tweets:** 391,275 (indicating some duplicates/retweets)\n",
    "    \n",
    "- **Average tweet length:** 8.5 words (45.8 characters)\n",
    "    \n",
    "- **Tweets containing emoji:** 95,341\n",
    "    \n",
    "- **Most common words:** “nvda”, “amd”, “buy”, “stock”, “market”, etc., reflecting a strong focus on equities and financial topics\n",
    "    \n",
    "\n",
    "### (2) Financial vs. Non-Financial Tweet Classification\n",
    "\n",
    "- **Financial tweets:** 60,990 (≈ 13%)\n",
    "    \n",
    "- **Non-financial tweets:** 404,035 (≈ 87%)\n",
    "    \n",
    "- Indicates that although most tweets mention NVDA, only a small fraction employ explicit financial terminology or express investment viewpoints.\n",
    "    \n",
    "\n",
    "### (3) Sentiment Analysis Results\n",
    "\n",
    "- **Financial tweets (FinBERT):** sentiment scores in the range [–1, 1], showing a reasonable distribution\n",
    "    \n",
    "- **Non-financial tweets (BERT & VADER):**\n",
    "    \n",
    "    - VADER scores in a narrow band [–0.06, 0.06]\n",
    "        \n",
    "    - BERT scores all returned `nan` (suggesting issues with the model or input data)\n",
    "        \n",
    "- **Daily sentiment series:** aggregated as daily averages to facilitate alignment with market data in downstream analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
